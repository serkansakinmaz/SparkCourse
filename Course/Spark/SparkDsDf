package org.spark

import org.apache.spark.sql.SparkSession
import org.apache.log4j.{Level, Logger}

private case class Person2(name:String, age:Long, city:String)

object SparkDsDf {

  def main(args: Array[String]): Unit = {

    val rootLogger = Logger.getRootLogger()
    rootLogger.setLevel(Level.ERROR)
    Logger.getLogger("org.apache.spark").setLevel(Level.ERROR)
    Logger.getLogger("org.spark-project").setLevel(Level.ERROR)
    val session = SparkSession.builder().master("local").
      appName("DataSet vs DataFrame").getOrCreate()

    import session.implicits._

    val personDf = session.read.json("/Users/serkan/Desktop/Course/Spark/data/peopleage.json")


    personDf.printSchema()
    personDf.show()

    println("----------------")
    personDf.foreach(row => {
      println("Age:" + row.getAs("age") + " Name:" + row.getAs("name"))
    })
    println("----------------")

    personDf.foreach(row =>{
      println("Age:" + row.getLong(0) + " Name:" + row.getString(2))
    })

    println("----------------")

    val personDs = personDf.as[Person2]
    personDs.show()
    personDs.printSchema()

    personDs.foreach(row =>{
      println("Name:" + row.name + ",Age:"+ row.age)
    })

  }

}